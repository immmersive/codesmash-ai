# The Minimal AGI Model

As opposed to narrow AI, the general artificial intelligence in question needs to learn on its own. This is the most fundamental difference between AI and AGI. In all current AI systems, especially the LLM-based ones, the AI system is trained on large amount of data. From our pragmatic understanding, it is clear what is actually going on. The system is primed to be able to solve a particular problem. It is trained on a set of data, which is specific to a certain problem domain. In other words, it is trained specifically with respect to the problem the system is trying to solve. This is a clear pragmatic approach. 

Once trained, the system will try to statistically find the correct solution, based on what it has seen from the training data. The problem lies in the fact that due to the NFL theorem, the system is now unable to solve any other type of problem, which it has not been trained on. As we have mentioned before, certain algorithms work better on certain problem domains, while they perform worse, on some other problem domains. Thus, if there was human input, which has trained the AI system on data, it has done so with respect to the knowledge the person already knew that the LLM.based approach will work well on this particular problem domain. Furthermore, if the system was trained on understanding images of dogs, then we are expected to ask the system if he can tell if a picture is of a dog or not.

But we are not going to expect the system to be able to tell if a picture it is shown is a car, if it has not been shown what a car is. Once the system has been trained, it will only be optimal for the domain it has been trained on. As opposed to human problem solvers, who even though don't know what the solution to a noven problem is, will go out of their way and find out how to solve a problem. We can then say, that humans either solve problems they know how to solve and if they don't know how to solve a problem, they create a new approach which can then solve the problem.

Our AGI system is therefore supposed to do the same thing. If it is tasked with solvign a problem it doesn't know how to solve, it needs to find an approach, i.e. and algorithm, which will be able to solve the problem. Otherwise, training they system upfront with the data, defeats the purpose of building an AGI system in teh first place. The idea behind AGI is that we construct a system which can solve problems that we humans can't solve. Current AI systems on the other hand, only solve problems we already know how to solve, but they retreive the information faster than humans.

We can explain this situation with the following example.

# Example

A calculator is a very simple computing device. It only has a few arithmetic instructions available and can perform computations on numbers of certain length. It can do so faster than an average person. Yet, it is a fact that all the computations which are performed by a calcualtor, already known to people. The information on how to perform the computation, came from people already, thus the calculator did not actually extract this information by itself. The information on how to perform the calculations, was transfered by the people who made teh calculator with a specific goal, for it to be used to make those calculations. Once completed, the calcualtor stays a closed system, unable to solve any other problems which have not been given to the calculator at the time of development. Clearly, our AGI system cannot work based on this principle, and has to be able to learn how to solve new tasks.

# The AGI Definition

It should be clear that the NFL theorem tells us that no algorithm outperforms random chance over all possible problems. Our AGI system needs to actually be able to in principle solve all possible problems. Not only that, but its supposed to do that better than random chance. It would then seem we are going against the NFL theorem as it stands. This is actually not the case because we have a time component in our system. Our system will be defined with a minimal set of instructions and a minimal dataset. It is not supposed to be able to solve any problems upfront. Instead, it will self-train itself to become good at solving specific problems. In other words, it will prime itself to statistically be able to solve certain problems better than random chance.

So, our definition of an AGI system is as follows.

> An AGI system is any computational system, which given enough resources, over time, can prime itself to solve any problem better than random chance on average.

Thus, we understand that what the AGI system is actually expected to do, is either solve the problem it knows how to solve, or create new optimal algorithms which can solve problems id does not know how to solve. We can define the process of AGI thinking as follows.

- The process is started
- The system is primed for exploration
- The system extracts information from the environment
- The system starts coming up with ideas based on the information
- It starts classifying ideas into patterns and relations
- Once presented with a task it scancs the problem
- It then tries to understand the problem with respect to its internal data
- It then uses the pragmatic method to solve the problem
- this means that it creates an optimization problem
- it then tries to find
- if it can solve the problem, it returns the result
- if it can't solve the problem it decides on the best course of action based on its internal data
- it then adjusts the internal knowledge based on the outcome
